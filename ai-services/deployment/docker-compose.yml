# AI4Thai Crop Guardian - AI Services Deployment
# Vision Service and Queue Worker for agricultural analysis

version: '3.8'

services:
  # Vision Service - Pest Detection + Disease Detection (H100 16GB Optimized)
  vision-service:
    build:
      context: ../vision-service
      dockerfile: Dockerfile
      target: ${BUILD_TARGET:-h100-optimized}
    container_name: team10-vision-service
    ports:
      - "${VISION_SERVICE_PORT:-2001}:2001"
    environment:
      # Model Configuration - H100 16GB Optimized
      - MODEL_CACHE_DIR=/app/models
      - HUGGINGFACE_HUB_CACHE=/app/models/hub
      - TRANSFORMERS_CACHE=/app/models/transformers

      # HuggingFace Configuration
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}
      - HF_HOME=/app/models

      # Service Configuration
      - SERVICE_HOST=0.0.0.0
      - SERVICE_PORT=2001
      - MAX_WORKERS=1
      - WORKER_TIMEOUT=300

      # Model Settings - Memory Optimized
      - PEST_DETECTION_MODEL=underdogquality/yolo11s-pest-detection
      - DISEASE_DETECTION_MODEL=YuchengShi/LLaVA-v1.5-7B-Plant-Leaf-Diseases-Detection
      - CONFIDENCE_THRESHOLD=0.01
      - MAX_IMAGE_SIZE=5242880  # Reduced to 5MB for memory efficiency
      - MAX_BATCH_SIZE=1

      # H100 16GB GPU Configuration
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - TORCH_CUDA_ARCH_LIST="9.0"
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048,expandable_segments:True
      - CUDA_LAUNCH_BLOCKING=0
      - TORCH_CUDNN_V8_API_ENABLED=1

      # Memory Optimization for 16GB VRAM
      - MODEL_MAX_LENGTH=512
      - GRADIENT_CHECKPOINTING=true
      - USE_FLASH_ATTENTION=true
      - TORCH_COMPILE=false  # Disable for stability

      # Performance - Conservative for Memory
      - OMP_NUM_THREADS=2
      - MKL_NUM_THREADS=2

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1

      # Redis for caching
      - REDIS_URL=${REDIS_URL:-redis://redis:6379}

      # Monitoring
      - ENABLE_METRICS=true
      - METRICS_PORT=9001
    volumes:
      - team10-root:/root
      - team10-data:/app/models
    networks:
      - ai-services-network
    deploy:
      resources:
        limits:
          memory: ${VISION_MEMORY_LIMIT:-4G}  # Aggressive reduction for memory optimization
        reservations:
          memory: ${VISION_MEMORY_RESERVATION:-2G}  # Minimal reservation
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2001/health"]
      interval: 30s
      timeout: 15s
      retries: 2  # Reduced retries
      start_period: 60s  # Reduced startup time
    restart: unless-stopped

  # Queue Worker Service - Vision Job Processing
  queue-worker:
    build:
      context: ../queue-worker
      dockerfile: Dockerfile
      target: ${BUILD_TARGET:-production}
    container_name: team10-queue-worker
    ports:
      - "${QUEUE_WORKER_PORT:-2003}:2003"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - VISION_SERVICE_URL=http://vision-service:2001
      - MAX_IMAGE_SIZE=10485760  # 10MB
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      - QUEUE_PORT=2003
    depends_on:
      - redis
      - vision-service
    volumes:
      - team10-root:/root
      - team10-data:/app/data
    networks:
      - ai-services-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2003/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Celery Worker - Background Processing
  celery-worker:
    build:
      context: ../queue-worker
      dockerfile: Dockerfile
      target: ${BUILD_TARGET:-production}
    container_name: team10-celery-worker
    command: celery -A tasks worker --loglevel=info --concurrency=2
    environment:
      - REDIS_URL=redis://redis:6379/0
      - VISION_SERVICE_URL=http://vision-service:2001
      - MAX_IMAGE_SIZE=10485760
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    depends_on:
      - redis
      - vision-service
    volumes:
      - team10-root:/root
      - team10-data:/app/data
    networks:
      - ai-services-network
    deploy:
      resources:
        limits:
          memory: 2G  # Reduced for queue worker
        reservations:
          memory: 512M  # Minimal for queue processing
    restart: unless-stopped

  # Celery Beat - Scheduled Tasks
  celery-beat:
    build:
      context: ../queue-worker
      dockerfile: Dockerfile
      target: ${BUILD_TARGET:-production}
    container_name: team10-celery-beat
    command: celery -A tasks beat --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
    depends_on:
      - redis
    volumes:
      - team10-root:/root
      - team10-data:/app/data
    networks:
      - ai-services-network
    restart: unless-stopped

  # Redis - Cache, Session Management, and Job Queue
  redis:
    image: redis:7-alpine
    container_name: team10-ai-redis
    command: redis-server --appendonly yes --maxmemory 4gb --maxmemory-policy allkeys-lru
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - team10-root:/root
      - team10-data:/data
    networks:
      - ai-services-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Load Balancer for Vision Service
  vision-lb:
    image: nginx:alpine
    container_name: team10-vision-lb
    ports:
      - "${VISION_LB_PORT:-2011}:80"
    volumes:
      - team10-root:/root
      - team10-data:/etc/nginx
    depends_on:
      - vision-service
    networks:
      - ai-services-network
    profiles:
      - load-balancer
    restart: unless-stopped

  # Monitoring - Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: team10-ai-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - team10-root:/root
      - team10-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - ai-services-network
    profiles:
      - monitoring
    restart: unless-stopped

  # Monitoring - Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: team10-ai-grafana
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - team10-root:/root
      - team10-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - ai-services-network
    profiles:
      - monitoring
    restart: unless-stopped

networks:
  ai-services-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  team10-root:
    driver: local
  team10-data:
    driver: local
