# AI4Thai Shared Python Service Dockerfile Template
# Multi-stage build optimized for Python services

# Build arguments
ARG PYTHON_VERSION=3.11
ARG SERVICE_NAME={{ SERVICE_NAME }}
ARG SERVICE_PORT={{ SERVICE_PORT }}

# Base stage with common setup
FROM python:${PYTHON_VERSION}-slim AS base

# Set environment variables for optimization
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    DEBIAN_FRONTEND=noninteractive \
    UV_NO_CACHE=1 \
    PYTHONFAULTHANDLER=1 \
    PYTHONHASHSEED=random

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    ca-certificates \
    {% if SERVICE_TYPE == "vision-service" %}
    libglib2.0-0 \
    libgomp1 \
    libgl1-mesa-glx \
    {% endif %}
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Install uv (Python package manager)
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.cargo/bin:$PATH"

# Create non-root user
RUN useradd --create-home --shell /bin/bash --uid 1000 app

# Set working directory
WORKDIR /app

# Copy uv configuration files for dependency resolution
COPY pyproject.toml .python-version ./

# Install dependencies using uv
RUN uv sync --frozen --no-dev

# Development stage
FROM base AS development

# Copy source code
COPY --chown=app:app . .

# Switch to non-root user
USER app

# Expose service port
EXPOSE ${SERVICE_PORT}

# Development command with hot reload
CMD ["uv", "run", "uvicorn", "app:app", "--host", "0.0.0.0", "--port", "${SERVICE_PORT}", "--reload"]

# Production stage
FROM base AS production

# Copy source code
COPY --chown=app:app . .

# Create necessary directories and set permissions
RUN mkdir -p /app/logs /app/data && \
    chown -R app:app /app && \
    # Cleanup for smaller image
    rm -rf /tmp/* /var/tmp/* /root/.cache/* && \
    find /app/.venv -name "*.pyc" -delete && \
    find /app/.venv -name "__pycache__" -type d -exec rm -rf {} + || true

# Switch to non-root user
USER app

# Expose service port
EXPOSE ${SERVICE_PORT}

# Health check
HEALTHCHECK --interval=30s --timeout=15s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:${SERVICE_PORT}/health || exit 1

# Production command
{% if SERVICE_TYPE == "vision-service" %}
CMD ["uv", "run", "gunicorn", "app:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:${SERVICE_PORT}", "--timeout", "300", "--max-requests", "100", "--preload"]
{% else %}
CMD ["uv", "run", "uvicorn", "app:app", "--host", "0.0.0.0", "--port", "${SERVICE_PORT}", "--workers", "1"]
{% endif %}

# GPU-optimized stage (for vision service)
{% if SERVICE_TYPE == "vision-service" %}
FROM production AS gpu-optimized

USER root

# Verify NVIDIA ML installation
RUN python -c "import pynvml; pynvml.nvmlInit(); print('NVML initialized successfully')" || \
    echo "Warning: NVML not available - running in CPU mode"

USER app

# GPU optimization environment variables
ENV CUDA_VISIBLE_DEVICES=0 \
    TORCH_CUDA_ARCH_LIST="9.0" \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048 \
    CUDA_LAUNCH_BLOCKING=0 \
    TORCH_CUDNN_V8_API_ENABLED=1 \
    MODEL_MAX_LENGTH=512 \
    MAX_BATCH_SIZE=1 \
    GRADIENT_CHECKPOINTING=true \
    USE_FLASH_ATTENTION=true

# GPU-optimized production command
CMD ["uv", "run", "gunicorn", "app:app", "-w", "1", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:${SERVICE_PORT}", "--timeout", "300", "--max-requests", "50", "--preload", "--worker-tmp-dir", "/dev/shm"]
{% endif %}
